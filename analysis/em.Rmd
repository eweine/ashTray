---
title: "An EM Algorithm for Separable Mixtures"
output:
  workflowr::wflow_html:
    code_folding: hide
date: "2025-01-27"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Simulation

```{r, results='hide'}
# I want to start to think about how to write an (approximate) EM algorithm for
# the mixture of separable normals case. I think that the approximate step
# should involve taking the TED of the sample covariance.
get_sep_weighted_loglik <- function(Y, omega_k, par, p1, p2, n_chol_R, n_chol_C) {

  L_R <- matrix(data = 0, nrow = p1, ncol = p1)
  L_C <- matrix(data = 0, nrow = p2, ncol = p2)
  L_R[lower.tri(L_R, diag = TRUE)] <- par[1:n_chol_R]
  L_C[lower.tri(L_C, diag = TRUE)] <- par[(n_chol_R + 1):(n_chol_R + n_chol_C)]

  R <- tcrossprod(L_R)
  C <- tcrossprod(L_C)
  Sigma <- kronecker(R, C)
  diag(Sigma) <- diag(Sigma) + 1

  chol_Sigma <- chol(Sigma)
  Sigma_inv <- chol2inv(chol_Sigma)
  half_logdet_Sigma <- sum(log(diag(chol_Sigma)))
  quad_form <- rowSums((Y %*% Sigma_inv) * Y)
  loglik <- -(sum(omega_k) * half_logdet_Sigma + 0.5 * sum(omega_k * quad_form))
  return(loglik)

}

optimize_RC_exact <- function(Y, R_start, C_start, p1, p2, omega_k) {

  #browser()
  L_R <- t(chol(R_start))
  L_C <- t(chol(C_start))

  chol_par_R <- L_R[lower.tri(L_R, diag = TRUE)]
  chol_par_C <- L_C[lower.tri(L_C, diag = TRUE)]

  opt_out <- optim(
    par = c(chol_par_R, chol_par_C),
    fn = get_sep_weighted_loglik,
    p1 = p1,
    p2 = p2,
    n_chol_R = length(chol_par_R),
    n_chol_C = length(chol_par_C),
    omega_k = omega_k,
    Y = Y,
    method = "L-BFGS-B",
    control = list(fnscale = -1, maxit = 5)
  )

  L_R[lower.tri(L_R, diag = TRUE)] <- head(opt_out$par, length(chol_par_R))
  L_C[lower.tri(L_C, diag = TRUE)] <- tail(opt_out$par, length(chol_par_C))

  return(
    list(
      R = tcrossprod(L_R),
      C = tcrossprod(L_C)
    )
  )

}


ted <- function (S, minval = 0, r = nrow(S)) {
  n <- nrow(S)
  r <- min(r,n)
  out <- eigen(S)
  d <- out$values
  d <- pmax(d-1,minval)
  if (r < n)
    d[seq(r+1,n)] <- minval
  return(tcrossprod(out$vectors %*% diag(sqrt(d))))
}

# Get the loglik of the model Y ~ N(0, I + R \kron C)
get_sep_loglik_per_pt <- function(Y, R, C) {

  Sigma <- kronecker(R, C)
  diag(Sigma) <- diag(Sigma) + 1
  chol_Sigma <- chol(Sigma)
  Sigma_inv <- chol2inv(chol_Sigma)
  half_logdet_Sigma <- sum(log(diag(chol_Sigma)))
  quad_form <- rowSums((Y %*% Sigma_inv) * Y)
  loglik <- -0.5 * quad_form - half_logdet_Sigma
  return(loglik)

}

get_null_loglik_per_pt <- function(Y) {

  -0.5 * rowSums(Y ^ 2)

}

# get responsibilities given mixture weights and components
# by default, make the last mixture weight correspond to the null
get_responsibilities <- function(Y, R_list, C_list, Pi) {

  K <- length(Pi)
  omega <- matrix(
    data = 0,
    nrow = nrow(Y),
    ncol = K
  )

  for (k in 1:(K - 1)) {

    omega[, k] <- Pi[k] * exp(
      get_sep_loglik_per_pt(Y, R_list[[k]], C_list[[k]])
    )

  }

  omega[, K] <- Pi[K] * exp(get_null_loglik_per_pt(Y))
  omega <- omega / rowSums(omega)

  return(omega)

}

# input vector of responsibilties
update_Pi <- function(omega) {

  return(
    colMeans(omega)
  )

}

get_S_list <- function(Y, omega) {

  K <- ncol(omega)
  S_list <- list()
  Y_T <- t(Y)

  for (k in 1:(K - 1)) {

    # compute t(Y) %*% diag(omega[,k]) %*% Y
    S_k <- crossprod(sqrt(omega[,k]) * Y) / sum(omega[,k])
    S_list[[k]] <- ted(S_k)

  }

  return(S_list)

}

get_R_given_C <- function(S, R, C) {

  n2 <- ncol(C)
  m2 <- nrow(C)

  # Only iterate over the upper triangle, including diagonal
  for (i in 1:nrow(R)) {
    for (j in i:ncol(R)) {

      S_ij <- S[
        ((i - 1) * m2 + 1):(i * m2),
        ((j - 1) * n2 + 1):(j * n2)
      ]

      # Compute once...
      val <- sum(S_ij * C)

      # ...and assign to both [i, j] and [j, i]
      R[i, j] <- val
      R[j, i] <- val
    }
  }

  # Scale by sum(C^2)
  R <- R / sum(C^2)
  return(R)
}

get_C_given_R <- function(S, R, C) {

  n1 <- ncol(R)
  m1 <- nrow(R)
  n2 <- ncol(C)
  m2 <- nrow(C)

  # Only iterate over the upper triangle, including diagonal
  for (i in 1:nrow(C)) {
    for (j in i:ncol(C)) {

      # Extract the sub-block of A relevant to (i,j)
      S_hat_ij <- S[
        i + (0:(m1 - 1)) * m2,
        j + (0:(n1 - 1)) * n2
      ]

      val <- sum(S_hat_ij * R)

      # Assign symmetrically
      C[i, j] <- val
      C[j, i] <- val
    }
  }

  # Scale by sum(R^2)
  C <- C / sum(R^2)
  return(C)
}


optimize_RC_alt_LS <- function(S, R_start, C_start, maxiter = 100, reltol = 1e-12) {

  R <- R_start
  C <- C_start
  old_obj <- norm(S - kronecker(R, C), type = "F")
  obj <- old_obj

  for (k in 1:maxiter) {

    R <- get_R_given_C(S, R, C)
    C <- get_C_given_R(S, R, C)

    obj <- norm(S - kronecker(R, C), type = "F")
    rel_improvement <- (old_obj - obj) / old_obj
    if (rel_improvement < reltol) {

      break

    } else {

      old_obj <- obj

    }

  }

  return(
    list(
      R = R,
      C = C
    )
  )

}

loglik_em <- function(Y, R_list, C_list, Pi) {

  K <- length(Pi)
  lik_mat <- matrix(
    data = 0,
    nrow = nrow(Y),
    ncol = K
  )

  for (k in 1:(K - 1)) {

    lik_mat[, k] <- Pi[k] * exp(
      get_sep_loglik_per_pt(Y, R_list[[k]], C_list[[k]])
    )

  }

  lik_mat[, K] <- Pi[K] * exp(get_null_loglik_per_pt(Y))
  ll <- sum(log(rowSums(lik_mat)))
  return(ll)

}

# presumably will need other tolerance parameters here
# a two component mixture contains one null and one non null
optim_em <- function(
    Y, p1, p2, K, maxiter = 100, m_step_method = c("exact", "approx")
) {

  m_step_method <- match.arg(m_step_method)
  # initialize parameters
  Pi <- rep(1 / K, K)
  R_list <- list()
  C_list <- list()

  for (k in 1:(K - 1)) {

    R_list[[k]] <- diag(p1)
    C_list[[k]] <- diag(p2)

  }

  loglik_vec <- c()
  loglik <- loglik_em(Y, R_list, C_list, Pi)
  loglik_vec <- c(loglik_vec, loglik)

  for (i in 1:maxiter) {

    print(glue::glue("Iteration {i}: loglik = {loglik}"))

    responsibilities <- get_responsibilities(Y, R_list, C_list, Pi)
    Pi <- update_Pi(responsibilities)
    # weighted empirical covariance matrices
    S_list <- get_S_list(Y, responsibilities)

    if (m_step_method == "approx") {

      # find closest separable covariance matrix for each mixture component
      for (k in 1:(K - 1)) {

        opt_RC <- optimize_RC_alt_LS(S_list[[k]], R_list[[k]], C_list[[k]])
        R_list[[k]] <- opt_RC$R
        C_list[[k]] <- opt_RC$C

      }

    } else {

      # Maximize log-likelihood
      for (k in 1:(K - 1)) {

        opt_RC <- optimize_RC_exact(
          Y, R_list[[k]], C_list[[k]], p1, p2, responsibilities[,k]
        )
        R_list[[k]] <- opt_RC$R
        C_list[[k]] <- opt_RC$C

      }

    }


    loglik <- loglik_em(Y, R_list, C_list, Pi)
    loglik_vec <- c(loglik_vec, loglik)

  }

  return(
    list(
      Pi = Pi,
      R_list = R_list,
      C_list = C_list,
      loglik = loglik_vec
    )
  )

}

# I think that the first thing to try is to generate data from a mixture
# of a point mass at 0 and a separable covariance structure
library(matrixsampling)

# s controls the portion of sparsity
generate_point_normal_sim <- function(n, p1, p2, s = 0.5) {

  R <- rwishart(n = 1, nu = 1, Sigma = diag(p1))[,,1]
  C <- rwishart(n = 1, nu = 1, Sigma = diag(p2))[,,1]
  Sigma <- kronecker(R, C)
  diag(Sigma) <- diag(Sigma) + 1

  n_zero <- rbinom(1, n, s)
  n_nz <- n - n_zero

  Y_zero <- MASS::mvrnorm(
    n = n_zero, mu = rep(0, p1 * p2), Sigma = diag(p1 * p2)
  )
  Y_nz <- MASS::mvrnorm(
    n = n_nz, mu = rep(0, p1 * p2), Sigma = Sigma
  )
  Y <- rbind(Y_zero, Y_nz)

  diag(Sigma) <- diag(Sigma) - 1

  return(
    list(
      Y = Y,
      pct_0 = n_zero / n,
      Sigma = Sigma
    )
  )

}

```

Below, I simulated data from the model

$$\textrm{vec}(Y_{1}), \dots, \textrm{vec}(Y_{n}) \sim \pi_{0}\delta_{0} + (1 - \pi_{0})\mathcal{N}(0, I + R \otimes C),$$
where $\pi_{0} = 0.5$, and $R$ and $C$ are both $10 \times 10$ PSD matrices. I consider both an approximate and an exact EM algorithm, where the approximate algorithm minimizes the difference between the Frobenius norm of the weighted (based on the E-step) sample covariance matrix and a separable covariance. The exact approach directly optimizes the weighted log-likelihood of $R$ and $C$ in the M-step.

```{r, results='hide'}
set.seed(489)
n <- 250
p1 <- 10
p2 <- 10
s <- 0.5
dat <- generate_point_normal_sim(n, p1, p2, s)
opt_out <- optim_em(dat$Y, p1, p2, 2, 25, m_step_method = "approx")
opt_out2 <- optim_em(dat$Y, p1, p2, 2, 25, m_step_method = "exact")
```

Below are the log-likelihood plots for each method:

```{r}
ll_df <- data.frame(
  loglik = c(opt_out$loglik, opt_out2$loglik),
  method = c(
    rep("Approximate", length(opt_out$loglik)), 
    rep("Exact", length(opt_out$loglik))
  ),
  iteration = c(0:25, 0:25)
)

library(ggplot2)
ggplot(data = ll_df, aes(x = iteration, y = loglik)) +
  geom_line(aes(color = method)) +
  cowplot::theme_cowplot()
```

The final log-likelihoods are quite close, though clearly the exact method performs slightly better.

Let's look at the actual estimates. First, the covariance matrix:

```{r}
Sigma_out <- kronecker(opt_out$R_list[[1]], opt_out$C_list[[1]])
Sigma_out2 <- kronecker(opt_out2$R_list[[1]], opt_out2$C_list[[1]])

df <- data.frame(
  true = as.vector(dat$Sigma),
  frob = as.vector(Sigma_out),
  mle = as.vector(Sigma_out2)
)

xmin <- min(df$true, df$frob, df$mle)
xmax <- max(df$true, df$frob, df$mle)

g1 <- ggplot(data = df, aes(x = true, y = frob)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  xlim(xmin, xmax) +
  ylim(xmin, xmax) +
  coord_fixed() +
  xlab("True") +
  ylab("Frobenius Approx")

g2 <- ggplot(data = df, aes(x = true, y = mle)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  xlim(xmin, xmax) +
  ylim(xmin, xmax) +
  coord_fixed() +
  xlab("True") +
  ylab("MLE")

g3 <- ggplot(data = df, aes(x = frob, y = mle)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  xlim(xmin, xmax) +
  ylim(xmin, xmax) +
  coord_fixed() +
  xlab("Frobenius Approx") +
  ylab("MLE")

library(ggpubr)
ggarrange(g1, g2, g3, nrow = 2, ncol = 2)
```

```{r}
frob_df <- expand.grid(1:(p1 * p2), 1:(p1 * p2))
colnames(frob_df) <- c("row", "col")
frob_df$cov <- as.vector(Sigma_out)
frob_df$method <- "frob"

mle_df <- expand.grid(1:(p1 * p2), 1:(p1 * p2))
colnames(mle_df) <- c("row", "col")
mle_df$cov <- as.vector(Sigma_out2)
mle_df$method <- "mle"

true_df <- expand.grid(1:(p1 * p2), 1:(p1 * p2))
colnames(true_df) <- c("row", "col")
true_df$cov <- as.vector(dat$Sigma)
true_df$method <- "true"

cov_df <- rbind(frob_df, mle_df, true_df)

ggplot(cov_df, aes(x = col, y = row, fill = cov)) +
  geom_tile() +
  # Use facet_wrap to create a separate plot for each method
  facet_wrap(~ method) +
  # Optional: adjust the color scale
  scale_fill_gradient2(
    low = "blue",
    mid = "white",
    high = "red",
    midpoint = 0
  ) +
  theme_minimal() +
  scale_y_reverse() +
  coord_fixed()
```

Second, the mixture components:

```{r}
df <- data.frame(
  pi0 = c(0.5, opt_out$Pi[1], opt_out2$Pi[2]),
  method = c("True", "Approximate", "Exact")
)

ggplot(df, aes(x = method, y = pi0)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Estimates of Null Proportion", x = "Method", y = "Pi0") +
  cowplot::theme_cowplot()
```

Here, we see that the exact method does slightly better.

## Conclusion

The above suggests to me that the approximation method seems promising in the context of an EM algorithm as well. One potential strategy would be to initialize the fit with the approximate method and then run the exact method, which would hopefully converge quickly because of a good initialization.
